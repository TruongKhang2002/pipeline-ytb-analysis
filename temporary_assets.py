import os
import json
import pandas as pd
import boto3
import subprocess
from typing import Dict
from sqlalchemy import create_engine
from dagster import asset, AssetExecutionContext
from kaggle.api.kaggle_api_extended import KaggleApi

@asset
def extract_data(context: AssetExecutionContext) -> list[str]:
    """Táº£i data tá»« Kaggle vÃ  upload lÃªn Minio"""
    download_path = "/tmp/raw"
    os.makedirs(download_path, exist_ok=True)

    # Authenticate Kaggle
    api = KaggleApi()
    api.authenticate()
    api.dataset_download_files("datasnaek/youtube-new", path=download_path, unzip=True)
    context.log.info("ÄÃ£ táº£i toÃ n bá»™ dataset tá»« Kaggle")

    context.log.info(f"AWS_ACCESS_KEY_ID: {os.getenv('AWS_ACCESS_KEY_ID', 'minio')}")
    context.log.info(f"AWS_SECRET_ACCESS_KEY: {os.getenv('AWS_SECRET_ACCESS_KEY', 'minio123')}")
    context.log.info(f"MINIO_ENDPOINT: {os.getenv('MINIO_ENDPOINT', 'http://minio:9000')}")
    # Khá»Ÿi táº¡o client MinIO
    s3 = boto3.client("s3",
        endpoint_url=os.getenv("MINIO_ENDPOINT", "http://localhost:9000"),
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID", "minio"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minio123"),
        #region_name="us-east-1"
    )

    #bucket = os.getenv("DATALAKE_BUCKET")
    bucket = os.getenv("DATALAKE_BUCKET", "warehouse")
    if not bucket:
        raise ValueError("MINIO bucket chÆ°a Ä‘Æ°á»£c Ä‘áº·t (DATALAKE_BUCKET is missing)")
    context.log.info(f"ðŸ“¦ Using bucket: {bucket}")
    try:
        s3.head_bucket(Bucket=bucket)
    except:
        s3.create_bucket(Bucket=bucket)

    uploaded_countries = []

    for file in os.listdir(download_path):
        if not isinstance(file, str):
            context.log.warning(f"Bá» qua non-string file: {file}")
            continue
        if file.endswith("videos.csv"):
            country = file[:2].upper()

            if not country.isalpha():
                context.log.warning(f"Bá» qua file khÃ´ng há»£p lá»‡: {file}")
                continue

            csv_path = os.path.join(download_path, file)
            json_name = f"{country}_category_id.json"
            json_path = os.path.join(download_path, json_name)

            # Upload CSV
            s3.upload_file(csv_path, bucket, f"bronze/{country}/{file}")
            context.log.info(f"â¬†ï¸ Uploaded {file} lÃªn MinIO")

            if os.path.exists(json_path):
                s3.upload_file(json_path, bucket, f"bronze/{country}/{json_name}")
                context.log.info(f"â¬†ï¸ Uploaded {json_name} lÃªn MinIO")
            else:
                context.log.warning(f"KhÃ´ng tÃ¬m tháº¥y {json_name}, sáº½ bá» qua Ã¡nh xáº¡ category_name")

            uploaded_countries.append(country)

    return uploaded_countries


@asset (
    name="load_data",
    description="Load data tá»« MinIO sang PostgreSQL",
    metadata={
        "source": "ðŸª£ MinIO",
        "destination": "ðŸ˜ PostgreSQL"
    }
)
def load_data(context: AssetExecutionContext, extract_data: list[str]):
    """Load data vÃ o PostgreSQL"""
    bucket = os.getenv("MINIO_BUCKET", "warehouse")
    s3 = boto3.client("s3",
        endpoint_url=os.getenv("MINIO_ENDPOINT", "http://localhost:9000"),
        aws_access_key_id=os.getenv("MINIO_ACCESS_KEY", "minio"),
        aws_secret_access_key=os.getenv("MINIO_SECRET_KEY", "minio123"))

    pg_user = os.getenv("POSTGRES_USER", "admin")
    pg_password = os.getenv("POSTGRES_PASSWORD", "admin123")
    pg_host = os.getenv("POSTGRES_HOST", "localhost")
    pg_port = os.getenv("POSTGRES_PORT", "5432")
    pg_db = os.getenv("POSTGRES_DB", "postgres")

    conn_info = (
        f"postgresql+psycopg2://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}"
    )
    engine = create_engine(conn_info)

    for country in extract_data:
        csv_key = f"bronze/{country}/{country}videos.csv"
        json_key = f"bronze/{country}/{country}_category_id.json"
        local_csv = f"/tmp/temp_{country}.csv"
        local_json = f"/tmp/temp_{country}.json"

        # Táº£i file CSV
        s3.download_file(bucket, csv_key, local_csv)

        # Äá»c dá»¯ liá»‡u
        try:
            df = pd.read_csv(local_csv, encoding="utf-8")
        except UnicodeDecodeError as e:
            
            df = pd.read_csv(local_csv, encoding="latin1")
        df["country"] = country

        # Náº¿u cÃ³ file JSON thÃ¬ Ã¡nh xáº¡ category
        
        try:
            s3.download_file(bucket, json_key, local_json)
            with open(local_json, "r", encoding="utf-8") as f:
                raw = f.read()
                
            categories = json.loads(raw)
            cat_map = {int(item["id"]): item["snippet"]["title"] for item in categories["items"]}
            
            #df["categoryId"] = pd.to_numeric(df["categoryId"], errors="coerce").astype("Int64")
            #df["category_name"] = df["categoryId"].map(cat_map)
            df["categoryId"] = pd.to_numeric(df["category_id"], errors="coerce")
            df["categoryId"] = df["categoryId"].astype("Int64")
            df["category_name"] = df["categoryId"].apply(lambda x: cat_map.get(int(x)) if pd.notna(x) else None)
            

            context.log.info(f"Ãnh xáº¡ category cho {country}")
        except:
            context.log.warning(f"KhÃ´ng Ã¡nh xáº¡ Ä‘Æ°á»£c category_name cho {country}")
            df["category_name"] = None
        

        # Ghi vÃ o PostgreSQL
        table_name = f"trending_{country.lower()}_raw"
        df.to_sql(table_name, engine, if_exists="append", index=False)
        context.log.info(f"ÄÃ£ insert {len(df)} records vÃ o báº£ng {table_name}")


@asset
def transform_data(context: AssetExecutionContext, load_data):
    """Cháº¡y DBT transform"""
    result = subprocess.run(
        ["dbt", "run", "--profiles-dir", "./", "--project-dir", "youtube_trending"],
        capture_output=True,
        text=True
    )
    context.log.info(result.stdout)
    if result.returncode != 0:
        raise Exception("DBT run failed")

#==================================================
import os
import json
import pandas as pd
import subprocess
from dagster import AssetExecutionContext, asset
from sqlalchemy import create_engine


@asset(io_manager_key="minio_io_manager")
def extract_data(context: AssetExecutionContext) -> dict:
    """Táº£i dá»¯ liá»‡u tá»« Kaggle vÃ  lÆ°u lÃªn MinIO."""
    from kaggle.api.kaggle_api_extended import KaggleApi
    import boto3

    download_path = "/tmp/raw"
    os.makedirs(download_path, exist_ok=True)

    api = KaggleApi()
    api.authenticate()
    api.dataset_download_files("datasnaek/youtube-new", path=download_path, unzip=True)
    context.log.info("âœ… ÄÃ£ táº£i toÃ n bá»™ dataset tá»« Kaggle")

    s3 = boto3.client(
        "s3",
        endpoint_url=os.getenv("MINIO_ENDPOINT", "http://localhost:9000"),
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID", "minio"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minio123"),
    )
    bucket = os.getenv("DATALAKE_BUCKET", "warehouse")

    uploaded = {}

    for file in os.listdir(download_path):
        if file.endswith("videos.csv"):
            country = file[:2].upper()
            key = f"bronze/{country}/{file}"
            s3.upload_file(os.path.join(download_path, file), bucket, key)
            context.log.info(f"â¬†ï¸ Uploaded {file} to {key}")
            uploaded[country] = {"csv": key}

        if file.endswith("_category_id.json"):
            country = file[:2].upper()
            key = f"bronze/{country}/{file}"
            s3.upload_file(os.path.join(download_path, file), bucket, key)
            context.log.info(f"â¬†ï¸ Uploaded {file} to {key}")
            uploaded[country]["json"] = key

    return uploaded


@asset(io_manager_key="psql_io_manager")
def load_data(context: AssetExecutionContext, extract_data: dict) -> list[str]:
    """Táº£i file tá»« MinIO vÃ  load vÃ o PostgreSQL"""
    import boto3
    bucket = os.getenv("DATALAKE_BUCKET", "warehouse")

    s3 = boto3.client(
        "s3",
        endpoint_url=os.getenv("MINIO_ENDPOINT", "http://localhost:9000"),
        aws_access_key_id=os.getenv("MINIO_ACCESS_KEY", "minio"),
        aws_secret_access_key=os.getenv("MINIO_SECRET_KEY", "minio123"),
    )

    pg_user = os.getenv("POSTGRES_USER", "admin")
    pg_password = os.getenv("POSTGRES_PASSWORD", "admin123")
    pg_host = os.getenv("POSTGRES_HOST", "localhost")
    pg_port = os.getenv("POSTGRES_PORT", "5432")
    pg_db = os.getenv("POSTGRES_DB", "postgres")
    conn_str = f"postgresql+psycopg2://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}"

    engine = create_engine(conn_str)

    for country, paths in extract_data.items():
        local_csv = f"/tmp/{country}.csv"
        s3.download_file(bucket, paths["csv"], local_csv)

        df = pd.read_csv(local_csv, encoding="latin1")
        df["country"] = country

        if "json" in paths:
            local_json = f"/tmp/{country}.json"
            s3.download_file(bucket, paths["json"], local_json)
            with open(local_json, "r", encoding="utf-8") as f:
                categories = json.load(f)
            cat_map = {int(item["id"]): item["snippet"]["title"] for item in categories["items"]}
            df["categoryId"] = pd.to_numeric(df["category_id"], errors="coerce").astype("Int64")
            df["category_name"] = df["categoryId"].map(cat_map)
        else:
            df["category_name"] = None

        table_name = f"trending_{country.lower()}_raw"
        df.to_sql(table_name, engine, if_exists="replace", index=False)
        context.log.info(f"ðŸ“¥ Loaded {len(df)} rows to {table_name}")

    return list(extract_data.keys())



#===========================================
import os
import json
import pandas as pd
import subprocess
from typing import Dict
from dagster import multi_asset, AssetOut, AssetExecutionContext, asset, Output
from sqlalchemy import create_engine
from kaggle.api.kaggle_api_extended import KaggleApi
import boto3


@asset(io_manager_key="minio_io_manager")
def extract_data(context: AssetExecutionContext) -> dict:
    """Táº£i dá»¯ liá»‡u tá»« Kaggle vÃ  lÆ°u lÃªn MinIO."""
    

    download_path = "/tmp/raw"
    os.makedirs(download_path, exist_ok=True)

    api = KaggleApi()
    api.authenticate()
    api.dataset_download_files("datasnaek/youtube-new", path=download_path, unzip=True)
    context.log.info("âœ… ÄÃ£ táº£i toÃ n bá»™ dataset tá»« Kaggle")

    s3 = boto3.client(
        "s3",
        endpoint_url=os.getenv("MINIO_ENDPOINT", "http://localhost:9000"),
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID", "minio"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minio123"),
    )
    bucket = os.getenv("DATALAKE_BUCKET", "warehouse")

    uploaded = {}

    for file in os.listdir(download_path):
        if file.endswith("videos.csv"):
            country = file[:2].upper()
            key = f"bronze/{country}/{file}"
            s3.upload_file(os.path.join(download_path, file), bucket, key)
            context.log.info(f"â¬†ï¸ Uploaded {file} to {key}")
            uploaded[country] = {"csv": key}

        if file.endswith("_category_id.json"):
            country = file[:2].upper()
            key = f"bronze/{country}/{file}"
            s3.upload_file(os.path.join(download_path, file), bucket, key)
            context.log.info(f"â¬†ï¸ Uploaded {file} to {key}")
            uploaded[country]["json"] = key

    return uploaded

@asset(io_manager_key="psql_io_manager")
def load_data(context: AssetExecutionContext, extract_data: Dict) -> None:
    """Load toÃ n bá»™ cÃ¡c file CSV/JSON tá»« MinIO vÃ o PostgreSQL"""
    import os
    import json
    import pandas as pd
    import boto3
    from sqlalchemy import create_engine

    bucket = os.getenv("DATALAKE_BUCKET", "warehouse")
    s3 = boto3.client(
        "s3",
        endpoint_url=os.getenv("MINIO_ENDPOINT", "http://localhost:9000"),
        aws_access_key_id=os.getenv("MINIO_ACCESS_KEY", "minio"),
        aws_secret_access_key=os.getenv("MINIO_SECRET_KEY", "minio123"),
    )

    conn_str = (
        f"postgresql+psycopg2://{os.getenv('POSTGRES_USER', 'admin')}:" 
        f"{os.getenv('POSTGRES_PASSWORD', 'admin123')}@" 
        f"{os.getenv('POSTGRES_HOST', 'localhost')}:" 
        f"{os.getenv('POSTGRES_PORT', '5432')}/"
        f"{os.getenv('POSTGRES_DB', 'postgres')}"
    )
    engine = create_engine(conn_str)

    for country, paths in extract_data.items():
        local_csv = f"/tmp/{country}.csv"
        s3.download_file(bucket, paths["csv"], local_csv)
        df = pd.read_csv(local_csv, encoding="latin1")
        df["country"] = country.upper()

        if "json" in paths:
            local_json = f"/tmp/{country}.json"
            s3.download_file(bucket, paths["json"], local_json)
            with open(local_json, "r", encoding="utf-8") as f:
                categories = json.load(f)
            cat_map = {int(item["id"]): item["snippet"]["title"] for item in categories["items"]}
            df["categoryId"] = pd.to_numeric(df["category_id"], errors="coerce").astype("Int64")
            df["category_name"] = df["categoryId"].map(cat_map)
        else:
            df["category_name"] = None

        table_name = f"trending_{country.lower()}_raw"
        df.to_sql(table_name, engine, if_exists="replace", index=False)
        context.log.info(f"ðŸ“¥ Loaded {len(df)} rows into {table_name}")
'''
@multi_asset(
    outs={
        f"trending_{country}_raw": AssetOut(
            key=["public", f"trending_{country}_raw"],
            io_manager_key="psql_io_manager"
        )
        for country in ["ca", "de", "fr", "gb", "in", "jp", "kr", "mx", "ru", "us"]
    },
    can_subset=True
)
def load_data(context: AssetExecutionContext, extract_data: dict):
    """Táº£i file tá»« MinIO vÃ  load vÃ o PostgreSQL"""
    bucket = os.getenv("DATALAKE_BUCKET", "warehouse")
    s3 = boto3.client(
        "s3",
        endpoint_url=os.getenv("MINIO_ENDPOINT", "http://localhost:9000"),
        aws_access_key_id=os.getenv("MINIO_ACCESS_KEY", "minio"),
        aws_secret_access_key=os.getenv("MINIO_SECRET_KEY", "minio123"),
    )

    # Káº¿t ná»‘i DB
    conn_str = (
        f"postgresql+psycopg2://{os.getenv('POSTGRES_USER', 'admin')}:"
        f"{os.getenv('POSTGRES_PASSWORD', 'admin123')}@"
        f"{os.getenv('POSTGRES_HOST', 'localhost')}:"
        f"{os.getenv('POSTGRES_PORT', '5432')}/"
        f"{os.getenv('POSTGRES_DB', 'postgres')}"
    )
    engine = create_engine(conn_str)

    # Duyá»‡t qua cÃ¡c quá»‘c gia cÃ³ dá»¯ liá»‡u
    for country, paths in extract_data.items():
        country_lc = country.lower()
        local_csv = f"/tmp/{country}.csv"
        s3.download_file(bucket, paths["csv"], local_csv)

        df = pd.read_csv(local_csv, encoding="latin1")
        df["country"] = country.upper()

        # Xá»­ lÃ½ JSON náº¿u cÃ³
        if "json" in paths:
            local_json = f"/tmp/{country}.json"
            s3.download_file(bucket, paths["json"], local_json)
            with open(local_json, "r", encoding="utf-8") as f:
                categories = json.load(f)
            cat_map = {int(item["id"]): item["snippet"]["title"] for item in categories["items"]}
            df["categoryId"] = pd.to_numeric(df["category_id"], errors="coerce").astype("Int64")
            df["category_name"] = df["categoryId"].map(cat_map)
        else:
            df["category_name"] = None

        table_name = f"trending_{country_lc}_raw"
        df.to_sql(table_name, engine, if_exists="replace", index=False)

        context.log.info(f"ðŸ“¥ Loaded {len(df)} rows to {table_name}")

        # Yield asset theo tá»«ng quá»‘c gia
        yield Output(
            value=df,
            output_name=f"trending_{country_lc}_raw",
            metadata={"rows": len(df), "columns": list(df.columns)}
        )
'''